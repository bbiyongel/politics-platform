{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('ner_data_v1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플 개수: 1767380\n"
     ]
    }
   ],
   "source": [
    "func = lambda temp: [(w, t) for w, t in zip(temp[\"Word\"].values.tolist(), temp[\"Tag\"].values.tolist())]\n",
    "tagged_sentences=[t for t in data.groupby(\"Sentence: #\").apply(func)]\n",
    "print(\"전체 샘플 개수: {}\".format(len(tagged_sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences, ner_tags = [], [] \n",
    "for tagged_sentence in tagged_sentences[:22000]:\n",
    "    sentence, tag_info = zip(*tagged_sentence) \n",
    "    sentences.append(list(sentence)) \n",
    "    ner_tags.append(list(tag_info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_tokenizer = Tokenizer(num_words = 5000, oov_token='OOV', lower=False)\n",
    "src_tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "tar_tokenizer = Tokenizer(lower=False) \n",
    "tar_tokenizer.fit_on_texts(ner_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합의 크기 : 136999\n",
      "개체명 태깅 정보 집합의 크기 : 6\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(src_tokenizer.word_index) + 1\n",
    "tag_size = len(tar_tokenizer.word_index) + 1\n",
    "\n",
    "print('단어 집합의 크기 : {}'.format(vocab_size))\n",
    "print('개체명 태깅 정보 집합의 크기 : {}'.format(tag_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 1, '인물': 2, '기관/집단': 3, '장소': 4, '정당': 5}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tar_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = src_tokenizer.texts_to_sequences(sentences)\n",
    "y_train = tar_tokenizer.texts_to_sequences(ner_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_word = src_tokenizer.index_word\n",
    "index_to_ner = tar_tokenizer.index_word\n",
    "index_to_ner[0]='PAD'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'O', 2: '인물', 3: '기관/집단', 4: '장소', 5: '정당', 0: 'PAD'}\n"
     ]
    }
   ],
   "source": [
    "print(index_to_ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기존의 문장 : ['[', '국회', '초선이', '바꾼다', ']', '⑥', '미래통합당', '김웅', '(', '서울', '송파갑', ')', '김종인', '비대위', '전환', '불가피', '판단', '“', '한국당엔', '정공법으로', '”', '합당', '지지', '“', '청년이', '주인', '되는', '정당', '만들고파', '”', '김웅', '미래통합당', '당선인']\n",
      "디코딩 문장 : ['[', '국회', 'OOV', 'OOV', ']', 'OOV', 'OOV', 'OOV', '(', '서울', 'OOV', ')', '김종인', 'OOV', 'OOV', 'OOV', '판단', '“', 'OOV', 'OOV', '”', 'OOV', '지지', '“', 'OOV', 'OOV', '되는', '정당', 'OOV', '”', 'OOV', 'OOV', 'OOV']\n"
     ]
    }
   ],
   "source": [
    "decoded = []\n",
    "for index in X_train[0] : \n",
    "    decoded.append(index_to_word[index]) \n",
    "\n",
    "print('기존의 문장 : {}'.format(sentences[0]))\n",
    "print('디코딩 문장 : {}'.format(decoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 128\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=max_len)\n",
    "y_train = pad_sequences(y_train, padding='post', maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=.2, random_state=777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train, num_classes=tag_size)\n",
    "y_test = to_categorical(y_test, num_classes=tag_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 샘플 문장의 크기 : (41600, 128)\n",
      "훈련 샘플 레이블의 크기 : (41600, 128, 6)\n",
      "테스트 샘플 문장의 크기 : (10400, 128)\n",
      "테스트 샘플 레이블의 크기 : (10400, 128, 6)\n"
     ]
    }
   ],
   "source": [
    "print('훈련 샘플 문장의 크기 : {}'.format(X_train.shape))\n",
    "print('훈련 샘플 레이블의 크기 : {}'.format(y_train.shape))\n",
    "print('테스트 샘플 문장의 크기 : {}'.format(X_test.shape))\n",
    "print('테스트 샘플 레이블의 크기 : {}'.format(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "from keras_contrib.layers import CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=20, input_length=max_len, mask_zero=True))\n",
    "model.add(Bidirectional(LSTM(units=50, return_sequences=True, recurrent_dropout=0.1)))\n",
    "model.add(TimeDistributed(Dense(50, activation=\"relu\")))\n",
    "crf = CRF(tag_size)\n",
    "model.add(crf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 37440 samples, validate on 4160 samples\n",
      "Epoch 1/10\n",
      "37440/37440 [==============================] - 222s 6ms/step - loss: 21.7974 - crf_viterbi_accuracy: 0.9945 - val_loss: 21.9350 - val_crf_viterbi_accuracy: 0.9923\n",
      "Epoch 2/10\n",
      "37440/37440 [==============================] - 233s 6ms/step - loss: 21.7972 - crf_viterbi_accuracy: 0.9946 - val_loss: 21.9371 - val_crf_viterbi_accuracy: 0.9921\n",
      "Epoch 3/10\n",
      "37440/37440 [==============================] - 217s 6ms/step - loss: 21.7969 - crf_viterbi_accuracy: 0.9946 - val_loss: 21.9375 - val_crf_viterbi_accuracy: 0.9919\n",
      "Epoch 4/10\n",
      "24704/37440 [==================>...........] - ETA: 1:15 - loss: 21.7933 - crf_viterbi_accuracy: 0.9947"
     ]
    }
   ],
   "source": [
    "#model.compile(optimizer=\"rmsprop\", loss=crf.loss_function, metrics=[crf.accuracy])\n",
    "history = model.fit(X_train, y_train, batch_size=32, epochs=10, validation_split=0.1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           precision    recall  f1-score   support\n",
      "\n",
      "       인물       0.95      0.94      0.95      5962\n",
      "       장소       0.96      0.83      0.89      1661\n",
      "    기관/집단       0.90      0.84      0.87      1987\n",
      "       정당       1.00      0.98      0.99       579\n",
      "\n",
      "micro avg       0.95      0.90      0.93     10189\n",
      "macro avg       0.95      0.90      0.92     10189\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "def sequences_to_tag(sequences): # 예측값을 index_to_tag를 사용하여 태깅 정보로 변경하는 함수.\n",
    "    result = []\n",
    "    for sequence in sequences: # 전체 시퀀스로부터 시퀀스를 하나씩 꺼낸다.\n",
    "        temp = []\n",
    "        for pred in sequence: # 시퀀스로부터 예측값을 하나씩 꺼낸다.\n",
    "            pred_index = np.argmax(pred) # 예를 들어 [0, 0, 1, 0 ,0]라면 1의 인덱스인 2를 리턴한다.\n",
    "            temp.append(index_to_ner[pred_index].replace(\"PAD\", \"O\")) # 'PAD'는 'O'로 변경\n",
    "        result.append(temp)\n",
    "    return result\n",
    "\n",
    "y_predicted = model.predict(X_test)\n",
    "pred_tags = sequences_to_tag(y_predicted)\n",
    "test_tags = sequences_to_tag(y_test)\n",
    "\n",
    "print(classification_report(test_tags, pred_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from requests import get\n",
    "\n",
    "url = 'https://search.naver.com/search.naver?sm=tab_hty.top&where=news&query=윤미향+위안부'\n",
    "response = get(url)\n",
    "\n",
    "url_list = []\n",
    "html_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "for  a in html_soup.find_all('a', class_ = ' _sp_each_title'):\n",
    "    url_list.append(a['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from newspaper import Article\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "news = []\n",
    "for url in url_list:\n",
    "\n",
    "    article = Article(url)\n",
    "    article.download()\n",
    "    article.parse()\n",
    "    news.append(' '.join(word_tokenize(article.text)).replace('?','.').replace('!','.').split('.'))\n",
    "\n",
    "news = [n for n in news if len(n)>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "for text in news:\n",
    "    sentences= text\n",
    "    entity = []\n",
    "\n",
    "    for sent in sentences:\n",
    "        new_sentence = sent.split()\n",
    "\n",
    "        word_to_index = src_tokenizer.word_index\n",
    "        new_X=[]\n",
    "        for w in new_sentence:\n",
    "            try:\n",
    "                new_X.append(word_to_index.get(w,1))\n",
    "            except KeyError:\n",
    "                new_X.append(word_to_index['OOV'])\n",
    "\n",
    "        pad_new = pad_sequences([new_X], padding=\"post\", value=0, maxlen=128)\n",
    "        p = model.predict(np.array([pad_new[0]]))\n",
    "        p = np.argmax(p, axis=-1)\n",
    "\n",
    "        for w, pred in zip(new_sentence, p[0]):\n",
    "            label = tar_tokenizer.index_word[pred]\n",
    "            if label != 'O':\n",
    "                entity.append((w, label))\n",
    "\n",
    "    print('인물: {}'.format(', '.join(list(set([x[0] for x in set(entity) if len(x[0])>1 and len(x[0])<=3 and x[1]=='인물'])))))\n",
    "    print('정당: {}'.format(', '.join(list(set([x[0] for x in set(entity) if len(x[0])>1 and x[1]=='정당'])))))\n",
    "    print('기관/집단: {}'.format(', '.join(list(set([x[0] for x in set(entity) if len(x[0])>1 and x[1]=='기관/집단'])))))\n",
    "    print('장소: {}'.format(', '.join(list(set([x[0] for x in set(entity) if len(x[0])>1 and x[1]=='장소'])))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from newspaper import Article\n",
    "\n",
    "article = Article('https://news.chosun.com/site/data/html_dir/2020/06/12/2020061202622.html')\n",
    "article.download()\n",
    "article.parse()\n",
    "text = article.text\n",
    "sentences = ' '.join(word_tokenize(text)).replace('?', '.').replace('!', '.').split('.')\n",
    "entity = []\n",
    "\n",
    "for sent in sentences:\n",
    "    new_sentence = sent.split(' ')\n",
    "    \n",
    "    word_to_index = src_tokenizer.word_index\n",
    "    new_X=[]\n",
    "    for w in new_sentence:\n",
    "        try:\n",
    "            new_X.append(word_to_index.get(w,1))\n",
    "        except KeyError:\n",
    "            new_X.append(word_to_index['OOV'])\n",
    "            \n",
    "    pad_new = pad_sequences([new_X], padding=\"post\", value=0, maxlen=max_len)\n",
    "    p = model.predict(np.array([pad_new[0]]))\n",
    "    p = np.argmax(p, axis=-1)\n",
    "\n",
    "    for w, pred in zip(new_sentence, p[0]):\n",
    "        label = tar_tokenizer.index_word[pred]\n",
    "        if label != 'O':\n",
    "            entity.append((w, label))\n",
    "            \n",
    "print(set(entity))\n",
    "#print(set([x for x in entity if entity.count(x)>1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from requests import get\n",
    "\n",
    "url = 'https://search.naver.com/search.naver?sm=tab_hty.top&where=news&query=자사고+폐지'\n",
    "response = get(url)\n",
    "\n",
    "url_list = []\n",
    "html_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "for  a in html_soup.find_all('a', class_ = ' _sp_each_title')[:4]:\n",
    "    url_list.append(a['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from newspaper import Article\n",
    "\n",
    "entity = []\n",
    "for url in url_list:\n",
    "\n",
    "    article = Article(url)\n",
    "    article.download()\n",
    "    article.parse()\n",
    "    text = article.text\n",
    "    sentences = ' '.join(word_tokenize(text)).replace('?', '.').replace('!', '.').split('.')\n",
    "\n",
    "    for sent in sentences:\n",
    "        new_sentence = sent.split(' ')\n",
    "\n",
    "        word_to_index = src_tokenizer.word_index\n",
    "        new_X=[]\n",
    "        for w in new_sentence:\n",
    "            try:\n",
    "                new_X.append(word_to_index.get(w,1))\n",
    "            except KeyError:\n",
    "                new_X.append(word_to_index['OOV'])\n",
    "\n",
    "        pad_new = pad_sequences([new_X], padding=\"post\", value=0, maxlen=max_len)\n",
    "        p = model.predict(np.array([pad_new[0]]))\n",
    "        p = np.argmax(p, axis=-1)\n",
    "\n",
    "        for w, pred in zip(new_sentence, p[0]):\n",
    "            label = tar_tokenizer.index_word[pred]\n",
    "            if label != 'O':\n",
    "                entity.append((w, label))\n",
    "\n",
    "print('인물: {}'.format(', '.join(list(set([x[0] for x in set(entity) if len(x[0])>1 and x[1]=='인물'])))))\n",
    "print('직책: {}'.format(', '.join(list(set([x[0] for x in set(entity) if len(x[0])>1 and x[1]=='직책'])))))\n",
    "print('정당: {}'.format(', '.join(list(set([x[0] for x in set(entity) if len(x[0])>1 and x[1]=='정당'])))))\n",
    "print('기관/집단: {}'.format(', '.join(list(set([x[0] for x in set(entity) if len(x[0])>1 and x[1]=='기관/집단'])))))\n",
    "print('장소: {}'.format(', '.join(list(set([x[0] for x in set(entity) if len(x[0])>1 and x[1]=='장소'])))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# saving\n",
    "with open('politics_ner_src_tokenizer_v1.pickle', 'wb') as handle:\n",
    "    pickle.dump(src_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving\n",
    "with open('politics_ner_tar_tokenizer_v1.pickle', 'wb') as handle:\n",
    "    pickle.dump(tar_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"ner_model_v1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:hate_speech_topic]",
   "language": "python",
   "name": "conda-env-hate_speech_topic-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
